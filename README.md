# UDE-postgres

The goal of this project is to generate a PostgreSQL database that hosts structured data related to song and user activity with the purpose of having a playground where we can obtain quick insigts via SQL queries. In order to implement this we will organize the data from different datasets by means of an ETL procedure.

## Song and Log datasets

The data used in this project is contained in two datasets: the Song dataset and the Log dataset. Both datasets can be found in the directory `./data`. 

### Song dataset

The Song dataset is a subset of the [Million Song Dataset](http://millionsongdataset.com). The data is organized in with the following file structure 

```
./data/song_data/
└── A
    ├── A
    │   ├── A
    │   ├── B
    │   └── C
    └── B
        ├── A
        ├── B
        └── C
```

where the parameters of the song (song_id, title, duration, etc...) and the information of its artist (artist id, artist name, artist location, etc...) are written in JSON format. As an example, you can find below the data corresponding to the song `./song_data/A/A/A/TRAAAAW128F429D538.json`

```
{
   "num_songs":1,
   "artist_id":"ARD7TVE1187B99BFB1",
   "artist_latitude":null,
   "artist_longitude":null,
   "artist_location":"California - LA",
   "artist_name":"Casual",
   "song_id":"SOMZWCG12A8C13C480",
   "title":"I Didn't Mean To",
   "duration":218.93179,
   "year":0
}
```

### Log dataset

The Log dataset contains information related to the Song dataset generated by the simulator of events [Eventsim](https://github.com/Interana/eventsim). Each log file is written in JSON format labeled by date and contains similar information than the one present in activity logs from a music streaming application.  Below you can find an example that ilustrate the schema and information present in this data 

```
{
    "artist":"Des'ree",
    "auth":"Logged In", 
    "firstName":"Kaylee",
    "gender":"F",
    "itemInSession":1,
    "lastName":"Summers",
    "length":246.30812,
    "level":"free",
    "location":"Phoenix-Mesa-Scottsdale, AZ",
    "method":"PUT",
    "page":"NextSong",
    "registration":1540344794796.0,
    "sessionId":139,
    "song":"You Gotta Be",
    "status":200,
    "ts":1541106106796,
    "userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"",
    "userId":"8"}
```

## Database description

Since we are going to create the database from scratch we are going to design the database in a [normalized](https://en.wikipedia.org/wiki/Database_normalization) fashion following a [star schema](https://www.guru99.com/star-snowflake-data-warehousing.html) which is widely used while implementing OLTP transactions like DML statements. 
To this end, we are going to employ the `songplays`table as [fact table](https://en.wikipedia.org/wiki/Fact_table) `users`, `songs`, `artists` and `time` as [dimension tables](https://en.wikipedia.org/wiki/Dimension_(data_warehouse)). While the fact table encapsulated the information about the songs played by the users along time, the dimension ones are employed for descriptive purposes like song, user, artists and time related data. The fields of each one of the tables are detailed below.

#### `songplays` fact table

```
songplay_id SERIAL PRIMARY KEY,
start_time BIGINT, 
user_id INT, 
level VARCHAR, 
song_id VARCHAR, 
artist_id VARCHAR, 
session_id INT, 
location VARCHAR, 
user_agent VARCHAR
```

#### `users` dimension Tables

```
user_id INT PRIMARY KEY,
first_name VARCHAR,
last_name VARCHAR,
gender VARCHAR,
level VARCHAR
```

#### `songs` dimension Tables

```
song_id VARCHAR PRIMARY KEY,
title VARCHAR,
artist_id VARCHAR,
year INT,
duration FLOAT
```

#### `artists` dimension Tables

```
artist_id VARCHAR PRIMARY KEY,
artist_name VARCHAR,
artist_location VARCHAR,
artist_latitude INT,
artist_longitude INT
````

#### `time` dimension Tables

```
start_time BIGINT PRIMARY KEY,
hour INT,
day INT,
week INT,
month INT,
year INT,
weekday INT
```

As one can see in figure 1, the fact table is connected to the dimension ones with a foreign key that will make possible the use of JOIN statements to bring the data alltogether. 

INCLUDE FIGURE!

## ETL pipeline

## Example queries

## Requirements


