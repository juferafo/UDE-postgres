# UDE-postgres

The goal of this project is to generate a PostgreSQL database that hosts structured data related to song and user activity with the purpose of having a playground where we can obtain quick insigts via SQL queries. In order to implement this we will organize the data from different datasets by means of an ETL procedure.

## Song and Log datasets

The data used in this project is contained in two datasets: the Song dataset and the Log dataset. Both datasets can be found in the directory `./data`. 

### Song dataset

The Song dataset is a subset of the [Million Song Dataset](http://millionsongdataset.com). The data is organized in with the following file structure 

```
./data/song_data/
└── A
    ├── A
    │   ├── A
    │   ├── B
    │   └── C
    └── B
        ├── A
        ├── B
        └── C
```

where the parameters of the song (song_id, title, duration, etc...) and the information of its artist (artist id, artist name, artist location, etc...) are written in JSON format. As an example, you can find below the data corresponding to the song `./song_data/A/A/A/TRAAAAW128F429D538.json`

```
{
   "num_songs":1,
   "artist_id":"ARD7TVE1187B99BFB1",
   "artist_latitude":null,
   "artist_longitude":null,
   "artist_location":"California - LA",
   "artist_name":"Casual",
   "song_id":"SOMZWCG12A8C13C480",
   "title":"I Didn't Mean To",
   "duration":218.93179,
   "year":0
}
```

### Log dataset

The Log dataset contains information related to the Song dataset generated by the simulator of events [Eventsim](https://github.com/Interana/eventsim). Each log file is written in JSON format labeled by date and contains similar information than the one present in activity logs from a music streaming application.  Below you can find an example that ilustrate the schema and information present in this data 

```
{
    "artist":"Des'ree",
    "auth":"Logged In", 
    "firstName":"Kaylee",
    "gender":"F",
    "itemInSession":1,
    "lastName":"Summers",
    "length":246.30812,
    "level":"free",
    "location":"Phoenix-Mesa-Scottsdale, AZ",
    "method":"PUT",
    "page":"NextSong",
    "registration":1540344794796.0,
    "sessionId":139,
    "song":"You Gotta Be",
    "status":200,
    "ts":1541106106796,
    "userAgent":"\"Mozilla\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/35.0.1916.153 Safari\/537.36\"",
    "userId":"8"}
```

## Database description

Since we are going to create the database from scratch we are going to design the database in a [normalized](https://en.wikipedia.org/wiki/Database_normalization) fashion following a [star schema](https://www.guru99.com/star-snowflake-data-warehousing.html) which is widely used while implementing OLTP transactions like DML statements. 
To this end, we are going to employ the `songplays` table as [fact table](https://en.wikipedia.org/wiki/Fact_table) `users`, `songs`, `artists` and `time` as [dimension tables](https://en.wikipedia.org/wiki/Dimension_(data_warehouse)). While the fact table encapsulated the information about the songs played by the users along time, the dimension ones are employed for descriptive purposes like song, user, artists and time related data. The fields of each one of the tables are detailed below.

#### `songplays` fact table

```
songplay_id SERIAL PRIMARY KEY,
start_time BIGINT, 
user_id INT, 
level VARCHAR, 
song_id VARCHAR, 
artist_id VARCHAR, 
session_id INT, 
location VARCHAR, 
user_agent VARCHAR
```

#### `users` dimension Tables

```
user_id INT PRIMARY KEY,
first_name VARCHAR,
last_name VARCHAR,
gender VARCHAR,
level VARCHAR
```

#### `songs` dimension Tables

```
song_id VARCHAR PRIMARY KEY,
title VARCHAR,
artist_id VARCHAR,
year INT,
duration FLOAT
```

#### `artists` dimension Tables

```
artist_id VARCHAR PRIMARY KEY,
artist_name VARCHAR,
artist_location VARCHAR,
artist_latitude INT,
artist_longitude INT
````

#### `time` dimension Tables

```
start_time BIGINT PRIMARY KEY,
hour INT,
day INT,
week INT,
month INT,
year INT,
weekday INT
```

As one can see in figure 1, the fact table is connected to the dimension ones with a foreign key that will make possible the use of JOIN statements to bring the data alltogether. 

INCLUDE FIGURE!

## Database setup and data ETL pipeline

An automatized procedure was created in order to set-up the database with the aforementioned star-schema and integrate the song and log data in there. Such a procedure is carried out in two steps: the database generation and the data ingestion that are encapsulated in the python scripts `./create_tables.py` and `./etl.py` respectively. Below there is a description of how these stages work. 

1. **Database generation**: the script `./create_table.py` makes use of the python library [`psycopg2`](https://www.psycopg.org/docs/) to create both the database `sparkify` and the fact and dimensions tables by means of [`CREATE DATABASE`](https://www.postgresql.org/docs/9.0/sql-createdatabase.html) and [`CREATE TABLE`](https://www.postgresql.org/docs/10/sql-createtable.html) SQL DDL statements. As a measure of caution, before any `CREATE` operation a [`DROP DATABASE`](https://www.postgresql.org/docs/9.0/sql-dropdatabase.html) (or [`DROP TABLE`](https://www.postgresql.org/docs/10/sql-droptable.html)) statement is executed.

2. **Data ingestion**: the script `./etlp.py` contains the ETL (extract, transform, load) pipeline employed to ingest the song and log data into the `songplays`, `users`, `songs`, `artists` and `time` tables. Similarly to the Database generation step, the `psycopg2` library is used to set up a connecttion to the `sparkify` database and, after the data is extracted and transformed into the correct format, different [`INSERT`](https://www.postgresql.org/docs/current/sql-insert.html) SQL DML statements are employed to ingest data into the tables. This procedure is structured in two separate methods `process_song_file` and `process_log_file` according to the source of the raw data.

## Example queries

## Requirements

psycopg2
pandas


